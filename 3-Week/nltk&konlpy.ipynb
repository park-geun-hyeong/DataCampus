{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import & define Func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.5.2', '3.5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from requests import request, HTTPError\n",
    "from requests.compat import urljoin, urlparse, quote, unquote\n",
    "from time import sleep\n",
    "import re\n",
    "from urllib import parse\n",
    "import os\n",
    "\n",
    "from requests.sessions import Session\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import konlpy \n",
    "import nltk\n",
    "\n",
    "konlpy.__version__, nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    data = re.sub(r'\\b[a-z_][a-zA-Z_()]{4,}\\b', '', data) ## 반드시 소문자+_ 1글자로 시작, 영문 4글자 이상인 패턴 삭제 ==>javascript부분 제거\n",
    "    data = re.sub(r'\\b[(){}/]{2,}','',data) ## 함수 기호 삭제 ==> () {} 제거\n",
    "    data = re.sub(r'([.,?!])(\\w)','\\g<1> \\g<2>', data) ## 구두점 다음에 글자가 있는 경우 공백 추가 ==> .단어 를 . 단어 로 변환 \n",
    "    data = re.sub(r'\\s{2,}', '\\n', data) ## 공백 2개 이상인 경우 한개만 ==> \\n\\n\\n\\n\\n ==> \\n\n",
    "    data = data.strip() ## 양쪽 공백 제거\n",
    "    data = re.sub(r'[^\\s\\w\\d.,?!:\\'\\\"]', '', data) ## 공백 문자 숫자 구두점을 제외한 나머지 글자 삭제 (특수 문자 삭제)\n",
    "    data = '\\n'.join(data.splitlines()[1:]) ## 주석 삭제\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "\n",
    "for file in os.listdir('./TXT'):\n",
    "    with open('./TXT/'+file, 'rb') as f:\n",
    "        corpus.append(cleaning(f.read().decode('utf-8', errors='ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 125503, 28929, 13026)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('./TXT')), len(data), len(data.split()), len(set(data.split())) ## unique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram_umjeol(s, N=2): ## 2음절씩 끊어서 데이터 만들기\n",
    "    rst = []\n",
    "    for i in range(len(s)-(N-1)):\n",
    "        rst.append(s[i:i+N])\n",
    "    \n",
    "    return rst\n",
    "\n",
    "## 음절 수에 따라서 해당하는 단어 count 해주기\n",
    "\n",
    "unigram = defaultdict(int)\n",
    "for token in ngram_umjeol(data,1):\n",
    "    unigram[token] += 1 \n",
    "\n",
    "bigram = defaultdict(int)\n",
    "for token in ngram_umjeol(data):\n",
    "    bigram[token] += 1  \n",
    "        \n",
    "trigram = defaultdict(int)\n",
    "for token in ngram_umjeol(data,3):\n",
    "    trigram[token] += 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125503, 1050, 125502, 14111, 125501, 38769)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sum(unigram.values()), len(unigram), sum(bigram.values()), len(bigram), sum(trigram.values()), len(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language Model(Nl understanding ==> probabiliatice Model)\n",
    "- MLE, MAP 빈도만\n",
    "- P(A,B) ==> freq(Bigram)/freq(Unigram) ==> 1st MA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram과 trigram을 활용해 단어의 빈도수를 통한 중요도 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n",
      "민주주 0.025\n",
      "==============================\n",
      "민주노 0.4\n",
      "==============================\n",
      "민주당 0.5625\n",
      "==============================\n",
      "민주일 0.0125\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "start= '민주'\n",
    "\n",
    "for _ in range(10):\n",
    "    freq1 = bigram[start]\n",
    "    maxfreq = 0.0\n",
    "    for k in [i for i in trigram.keys() if i.startswith(start)]: ## start로 시작하는 2음절\n",
    "        freq2 = trigram[k]\n",
    "        prob = freq2 / freq1\n",
    "        \n",
    "        print(k, prob)\n",
    "        print('='*30)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konlpy ===> feature selection\n",
    "- 굴적어 (어미의 활용이 다양)\n",
    "- 어근, 어간, 형태소 + 어미\n",
    "- 밥 을/도/() ...\n",
    "\n",
    "\n",
    "===> nlp에서 자연어들을 feature로 삼기 위해서는 어떻한 방식으로 든지 token화를 진행하여야 한다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma, Komoran, Hannanum, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adjective': '형용사',\n",
       " 'Adverb': '부사',\n",
       " 'Alpha': '알파벳',\n",
       " 'Conjunction': '접속사',\n",
       " 'Determiner': '관형사',\n",
       " 'Eomi': '어미',\n",
       " 'Exclamation': '감탄사',\n",
       " 'Foreign': '외국어, 한자 및 기타기호',\n",
       " 'Hashtag': '트위터 해쉬태그',\n",
       " 'Josa': '조사',\n",
       " 'KoreanParticle': '(ex: ㅋㅋ)',\n",
       " 'Noun': '명사',\n",
       " 'Number': '숫자',\n",
       " 'PreEomi': '선어말어미',\n",
       " 'Punctuation': '구두점',\n",
       " 'ScreenName': '트위터 아이디',\n",
       " 'Suffix': '접미사',\n",
       " 'Unknown': '미등록어',\n",
       " 'Verb': '동사'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EC': '연결 어미',\n",
       " 'ECD': '의존적 연결 어미',\n",
       " 'ECE': '대등 연결 어미',\n",
       " 'ECS': '보조적 연결 어미',\n",
       " 'EF': '종결 어미',\n",
       " 'EFA': '청유형 종결 어미',\n",
       " 'EFI': '감탄형 종결 어미',\n",
       " 'EFN': '평서형 종결 어미',\n",
       " 'EFO': '명령형 종결 어미',\n",
       " 'EFQ': '의문형 종결 어미',\n",
       " 'EFR': '존칭형 종결 어미',\n",
       " 'EP': '선어말 어미',\n",
       " 'EPH': '존칭 선어말 어미',\n",
       " 'EPP': '공손 선어말 어미',\n",
       " 'EPT': '시제 선어말 어미',\n",
       " 'ET': '전성 어미',\n",
       " 'ETD': '관형형 전성 어미',\n",
       " 'ETN': '명사형 전성 어미',\n",
       " 'IC': '감탄사',\n",
       " 'JC': '접속 조사',\n",
       " 'JK': '조사',\n",
       " 'JKC': '보격 조사',\n",
       " 'JKG': '관형격 조사',\n",
       " 'JKI': '호격 조사',\n",
       " 'JKM': '부사격 조사',\n",
       " 'JKO': '목적격 조사',\n",
       " 'JKQ': '인용격 조사',\n",
       " 'JKS': '주격 조사',\n",
       " 'JX': '보조사',\n",
       " 'MA': '부사',\n",
       " 'MAC': '접속 부사',\n",
       " 'MAG': '일반 부사',\n",
       " 'MD': '관형사',\n",
       " 'MDN': '수 관형사',\n",
       " 'MDT': '일반 관형사',\n",
       " 'NN': '명사',\n",
       " 'NNB': '일반 의존 명사',\n",
       " 'NNG': '보통명사',\n",
       " 'NNM': '단위 의존 명사',\n",
       " 'NNP': '고유명사',\n",
       " 'NP': '대명사',\n",
       " 'NR': '수사',\n",
       " 'OH': '한자',\n",
       " 'OL': '외국어',\n",
       " 'ON': '숫자',\n",
       " 'SE': '줄임표',\n",
       " 'SF': '마침표, 물음표, 느낌표',\n",
       " 'SO': '붙임표(물결,숨김,빠짐)',\n",
       " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
       " 'SS': '따옴표,괄호표,줄표',\n",
       " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
       " 'UN': '명사추정범주',\n",
       " 'VA': '형용사',\n",
       " 'VC': '지정사',\n",
       " 'VCN': \"부정 지정사, 형용사 '아니다'\",\n",
       " 'VCP': \"긍정 지정사, 서술격 조사 '이다'\",\n",
       " 'VV': '동사',\n",
       " 'VX': '보조 용언',\n",
       " 'VXA': '보조 형용사',\n",
       " 'VXV': '보조 동사',\n",
       " 'XP': '접두사',\n",
       " 'XPN': '체언 접두사',\n",
       " 'XPV': '용언 접두사',\n",
       " 'XR': '어근',\n",
       " 'XSA': '형용사 파생 접미사',\n",
       " 'XSN': '명사파생 접미사',\n",
       " 'XSV': '동사 파생 접미사'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma = Kkma()\n",
    "kkma.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "han = Hannanum()\n",
    "kom = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '아버지가방에들어가신다.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'Noun'),\n",
       " ('가방', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('들어가신다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가방', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('들어가', 'VV'),\n",
       " ('시', 'EPH'),\n",
       " ('ㄴ다', 'EFN'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지가방에들어가', 'N'), ('이', 'J'), ('시ㄴ다', 'E'), ('.', 'S')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "han.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가방', 'NNP'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('시', 'EP'),\n",
       " ('ㄴ다', 'EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kom.pos(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw, kobill ## text datastes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  kolaw.open(kolaw.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18884, 4178, 2029)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(data.split()), len(set(data.split())) ## 데이터 개수, 어절 개수, 하나뿐인 어절 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8796"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(okt.morphs(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1364"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(okt.morphs(data))) ## unique한 어절들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "josa = [i for i in okt.pos(data) if i[1] =='Josa'] ## 조사  ## pos를 할경우 해당하는 품사를 함께 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('가', 'Josa'),\n",
       " ('고', 'Josa'),\n",
       " ('과', 'Josa'),\n",
       " ('까지', 'Josa'),\n",
       " ('까지는', 'Josa'),\n",
       " ('까지로', 'Josa'),\n",
       " ('나', 'Josa'),\n",
       " ('는', 'Josa'),\n",
       " ('도', 'Josa'),\n",
       " ('든지', 'Josa'),\n",
       " ('로', 'Josa'),\n",
       " ('를', 'Josa'),\n",
       " ('마다', 'Josa'),\n",
       " ('며', 'Josa'),\n",
       " ('부터', 'Josa'),\n",
       " ('서', 'Josa'),\n",
       " ('에', 'Josa'),\n",
       " ('에게', 'Josa'),\n",
       " ('에게는', 'Josa'),\n",
       " ('에게만', 'Josa'),\n",
       " ('에는', 'Josa'),\n",
       " ('에도', 'Josa'),\n",
       " ('에서', 'Josa'),\n",
       " ('에서는', 'Josa'),\n",
       " ('에서의', 'Josa'),\n",
       " ('에의', 'Josa'),\n",
       " ('여', 'Josa'),\n",
       " ('와', 'Josa'),\n",
       " ('와의', 'Josa'),\n",
       " ('으로', 'Josa'),\n",
       " ('으로는', 'Josa'),\n",
       " ('으로부터', 'Josa'),\n",
       " ('으로서', 'Josa'),\n",
       " ('으로서의', 'Josa'),\n",
       " ('으로써', 'Josa'),\n",
       " ('은', 'Josa'),\n",
       " ('을', 'Josa'),\n",
       " ('의', 'Josa'),\n",
       " ('이', 'Josa'),\n",
       " ('이고', 'Josa'),\n",
       " ('이나', 'Josa'),\n",
       " ('이다', 'Josa'),\n",
       " ('이며', 'Josa'),\n",
       " ('인', 'Josa'),\n",
       " ('하고', 'Josa'),\n",
       " ('하고는', 'Josa'),\n",
       " ('한', 'Josa')}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(josa) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 담화 => 문단 => 문장 => 어절(구문) => 품사 => 형태소 => 토큰화\n",
    "- 문장부터 tokenizing 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk\n",
    "\n",
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg', 'punkt', 'stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\park1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 357)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.splitlines()),len(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Jone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"don't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " 'mr',\n",
       " \"jone's\",\n",
       " 'orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWordTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  kolaw.open(kolaw.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2638, 2479, 1673, 11588)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data.split())), len(set(word_tokenize(data))), len(set(okt.morphs(data))), len(TreebankWordTokenizer().tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize ## nltk 패키지에서 문장 token화를 위한 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['His barber kept his word.',\n",
       " 'But keeping such a huge secret to himself was driving him crazy.',\n",
       " 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n",
       " 'He dug a hole in the midst of some reeds.',\n",
       " 'He looked about, to make sure no one was near.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-2.5.1-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: kss\n",
      "Successfully installed kss-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\park1\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss ## 한국어 문장 tokenize를 위한 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['딥 러닝 자연어 처리가 재미있기는 합니다.',\n",
       " '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.',\n",
       " '농담아니에요.',\n",
       " '이제 해보면 알걸요?']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kss.split_sentences('딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['유구한.', '역사와.', '전통에.빛나는 우리대한민국은.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"유구한. 역사와. 전통에.빛나는 우리대한민국은.\") ### . 구두점을 기준으로 문장을 분석한다(느낌,물음표, 공백, 마침표 등등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation ### 구두점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '\\n'.join([kobill.open(corpus).read() for corpus in kobill.fileids()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648, 4308, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(data)), len(data.splitlines()), len(kss.split_sentences(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(okt.morphs(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('의', 380),\n",
       " ('.', 357),\n",
       " ('에', 282),\n",
       " ('을', 211),\n",
       " ('\\n', 195),\n",
       " ('은', 179),\n",
       " ('제', 178),\n",
       " ('이', 176),\n",
       " ('한다', 155),\n",
       " ('·', 145)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.vocab().most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25670759436107327"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ text.vocab().freq(i[0]) for i in text.vocab().most_common()[:10]]) ## 상위 10개의 단어들이 차지하는 비율 ===> 별 의미 없는 애들이 너무많은 비울을 차지한다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text.dispersion_plot(['국가']) ## 국가라는 단어의 사용 시점 plot으로 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국회 국무회의 국민 정부 헌법재판소 타인 국무총리 대한민국 법률 공무원 근로 대통령 조국 질서 각인 개정 그 정당 전통문화\n",
      "인간\n"
     ]
    }
   ],
   "source": [
    "text.similar('국가') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의하지 아니하고는; 헌법재판소 재판관; 그러하지 아니하다; 인하여 불이익\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "\n",
    "for file in os.listdir('./TXT'):\n",
    "    with open('./TXT/'+file, 'rb') as f:\n",
    "        corpus.append(cleaning(f.read().decode('utf-8', errors='ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(465, 1987, 508)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.splitlines()), len(sent_tokenize(data)), len(kss.split_sentences(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32886, 52947, 33254, 29202)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(data)), len(okt.morphs(data)), len(wordpunct_tokenize(data)), len(text_to_word_sequence(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = word_tokenize(data)\n",
    "token2 = okt.morphs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12576, 7397)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(token1)), len(set(token2)) ## unique한 단어들의 개수 비교 ## 형태소 분석한 결과가 feature의 수가 적다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 2018),\n",
       " ('이', 1326),\n",
       " ('을', 1243),\n",
       " ('를', 886),\n",
       " ('의', 829),\n",
       " ('에', 802),\n",
       " (',', 720),\n",
       " ('가', 677),\n",
       " ('\"', 635),\n",
       " ('는', 631)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text(token2).vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4192494381173626"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([text.vocab().freq(i[0]) for i in text.vocab().most_common(100)]) ##  빈도수가 높은 별 의미없는 단어들의 비율이 매우 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram 어절단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_eojeol(token, n=2):\n",
    "    #token = ['<sos>'] + token+['<eos>'] \n",
    "    rst=[]\n",
    "    for i in range(len(token) - (n-1)):\n",
    "        rst.append(' '.join(token[i:i+1]))\n",
    "        \n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = defaultdict(lambda : 0)\n",
    "bigram = defaultdict(lambda : 0)\n",
    "trigram = defaultdict(lambda : 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sent_tokenize(data):## data를 각각의 문장으로 쪼개기\n",
    "    tokens = okt.morphs(s)\n",
    "    \n",
    "    for i in ngram_eojeol(['<sos>'] + tokens + ['<eos>'],1): ## unigram 만들기\n",
    "        unigram[i] += 1\n",
    "        \n",
    "    for i in ngram_eojeol(['<sos>'] + tokens + ['<eos>'],2): ## bigram 만들기\n",
    "        bigram[i] += 1\n",
    "        \n",
    "    for i in ngram_eojeol(['<sos>'] + tokens + ['<eos>'],3): ## trigram 만들기\n",
    "        trigram[i] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7395, 7394, 7393)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram), len(bigram), len(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, e = '<sos>', '<eos>' ## 문장의 시작과 끝 미리 변수에 넣어두기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(s = '<sos>', e = '<eos>'):\n",
    "    #s, e = '<sos>', '<eos>' ## 문장의 시작과 끝 미리 변수에 넣어두기\n",
    "    for _ in range(10):\n",
    "        print(s)\n",
    "        unigram[s]\n",
    "        freq_list = {}\n",
    "        \n",
    "        for k in [filter(lambda keys: keys.split()[0] == s, bigram.keys())]:\n",
    "            prob = bigram[k]/unigram[s]\n",
    "            freq_list[k] = prob\n",
    "        \n",
    "        \n",
    "        p = np.random.rand()\n",
    "        sum_p = 0.0\n",
    "        sorted_rst = sorted(freq_list.items(), key = lambda _:_[1], reverse=True)\n",
    "        \n",
    "        for _ in sorted_rst:\n",
    "            sum_p += _[1]\n",
    "            if sum_p >= p:\n",
    "                next_key = _[0]\n",
    "                break\n",
    "        \n",
    "        \n",
    "        s = next_key.split()[-1]\n",
    "        if s == e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing\n",
    "- 문장, 단어 (어절 , 형태소 , 품사) , n-gram(NLU, NLG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Normalization\n",
    " - segmenting/ tokenizing words from running text\n",
    " - normalizing word formats\n",
    " - segmenting sentences in running text\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(punctuation), punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[{}]'.format(re.escape(punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i skip\n",
      "you skip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['like']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=[]\n",
    "\n",
    "for i in 'i like you'.split():\n",
    "    if i in stopword:\n",
    "        print(i, 'skip')\n",
    "    else:\n",
    "        clean.append(i)\n",
    "        \n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = word_tokenize('i like you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list): ## stopword 함수를 정희하여 제거해주기\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "remove_stopwords(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
